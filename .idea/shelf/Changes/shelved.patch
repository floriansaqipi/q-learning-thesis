Index: q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nimport torch\r\nimport torch.nn.functional as functional\r\nimport torch.optim as optim\r\n\r\nfrom .agent import Agent\r\nfrom .network import ConvolutionalDeepQLearningNetwork\r\nfrom .utils import ReplayMemory, ComputeDevice, FramePreprocessor\r\nfrom .. import Constants\r\nfrom ..environment import Environment\r\n\r\n\r\nclass ConvolutionalDeepQLearningAgent(Agent):\r\n    def __init__(\r\n            self,\r\n            env: Environment,\r\n            learning_rate: float,\r\n            discount_factor: float,\r\n            interpolation_parameter: float,\r\n            start_epsilon: float,\r\n            epsilon_decay: float,\r\n            final_epsilon: float,\r\n            replay_memory: ReplayMemory\r\n    ):\r\n        super().__init__(env)\r\n        self.learning_rate = learning_rate\r\n        self.discount_factor = discount_factor\r\n        self.interpolation_parameter = interpolation_parameter\r\n        self.epsilon = start_epsilon\r\n        self.epsilon_decay = epsilon_decay\r\n        self.final_epsilon = final_epsilon\r\n        self.replay_memory = replay_memory\r\n        self.device = ComputeDevice.get_device()\r\n        self.action_space = self.env.get_action_space().n\r\n        self.q_network = ConvolutionalDeepQLearningNetwork(self.action_space).to(self.device)\r\n        self.target_q_network = ConvolutionalDeepQLearningNetwork(self.action_space).to(self.device)\r\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\r\n        self.replay_memory = replay_memory\r\n        self.frame_preprocessor = FramePreprocessor()\r\n\r\n    def get_action(self, obs):\r\n        obs = self.frame_preprocessor.preprocess(obs)\r\n\r\n        self.q_network.eval()\r\n        with torch.no_grad():\r\n            action_values = self.q_network(obs)\r\n        self.q_network.train()\r\n        if np.random.random() < self.epsilon:\r\n            return torch.randint(0, action_values.size(1), (1,)).item()\r\n        else:\r\n            return torch.argmax(action_values, dim=1).item()\r\n\r\n    def get_best_action(self, obs):\r\n        obs = self.frame_preprocessor.preprocess(obs)\r\n        self.q_network.eval()\r\n        with torch.no_grad():\r\n            action_values = self.q_network(obs)\r\n        self.q_network.train()\r\n        return torch.argmax(action_values, dim=1).item()\r\n\r\n    def update(self, obs, action, reward, terminated, next_obs):\r\n        obs = self.frame_preprocessor.preprocess(obs)\r\n        next_obs = self.frame_preprocessor.preprocess(next_obs)\r\n        self.replay_memory.append((obs, action, reward, terminated, next_obs))\r\n        if len(self.replay_memory.memory) > self.replay_memory.batch_size:\r\n            transitions = self.replay_memory.sample()\r\n            self.learn(transitions)\r\n            self.soft_update()\r\n\r\n    def learn(self, transitions):\r\n        obs_batch, actions_batch, reward_batch, terminated_batch, next_obs_batch = transitions\r\n        future_q_target_values, _ = self.target_q_network(next_obs_batch).detach().max(dim=1, keepdim=True)\r\n        q_target_values = reward_batch + ((~terminated_batch) * self.discount_factor * future_q_target_values)\r\n        q_values = self.q_network(obs_batch).gather(1, actions_batch)\r\n        loss = functional.mse_loss(q_values, q_target_values)\r\n        self.training_error.append(q_values)\r\n        self.optimizer.zero_grad()\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n    def soft_update(self):\r\n        for target_parameter, local_parameter in zip(self.target_q_network.parameters(), self.q_network.parameters()):\r\n            target_parameter.data.copy_(\r\n                self.interpolation_parameter * local_parameter +\r\n                    (1.0 - self.interpolation_parameter) * target_parameter)\r\n\r\n    def save_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        torch.save(self.q_network.state_dict(), file_full_path)\r\n\r\n    def load_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        self.q_network.load_state_dict(torch.load(file_full_path, weights_only=True))\r\n\r\n    def decay_epsilon(self):\r\n        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py b/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py
--- a/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py	(date 1738783580510)
@@ -6,7 +6,7 @@
 from .agent import Agent
 from .network import ConvolutionalDeepQLearningNetwork
 from .utils import ReplayMemory, ComputeDevice, FramePreprocessor
-from .. import Constants
+from ..constans import Constants
 from ..environment import Environment
 
 
@@ -40,6 +40,7 @@
 
     def get_action(self, obs):
         obs = self.frame_preprocessor.preprocess(obs)
+        obs = torch.tensor(np.array(obs), dtype=torch.float32, device=self.device).unsqueeze(0)
 
         self.q_network.eval()
         with torch.no_grad():
@@ -52,6 +53,8 @@
 
     def get_best_action(self, obs):
         obs = self.frame_preprocessor.preprocess(obs)
+        obs = torch.tensor(np.array(obs), dtype=torch.float32, device=self.device).unsqueeze(0)
+
         self.q_network.eval()
         with torch.no_grad():
             action_values = self.q_network(obs)
@@ -93,4 +96,7 @@
         self.q_network.load_state_dict(torch.load(file_full_path, weights_only=True))
 
     def decay_epsilon(self):
-        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)
\ No newline at end of file
+        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)
+
+    def end_of_episode_hook(self):
+        self.decay_epsilon()
\ No newline at end of file
Index: test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import gymnasium as gym\r\n\r\nenv = gym.make(\"Taxi-v3\", render_mode=\"human\")\r\nobservation, info = env.reset()\r\n\r\nepisode_over = False\r\nwhile True:\r\n    while not episode_over:\r\n        action = env.action_space.sample()  # agent policy that uses the observation and info\r\n        observation, reward, terminated, truncated, info = env.step(action)\r\n\r\n        episode_over = terminated or truncated\r\n\r\nenv.close()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test.py b/test.py
--- a/test.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/test.py	(date 1738528463989)
@@ -1,14 +1,9 @@
 import gymnasium as gym
+import ale_py
 
-env = gym.make("Taxi-v3", render_mode="human")
-observation, info = env.reset()
 
-episode_over = False
-while True:
-    while not episode_over:
-        action = env.action_space.sample()  # agent policy that uses the observation and info
-        observation, reward, terminated, truncated, info = env.step(action)
+gym.register_envs(ale_py)
 
-        episode_over = terminated or truncated
-
-env.close()
\ No newline at end of file
+env = gym.make("ALE/Breakout-v5", render_mode="rgb_array")
+print (env.spec.max_episode_steps)
+env.reset()
\ No newline at end of file
Index: q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>LEARNING_RATE = 0.0005\r\nDISCOUNT_FACTOR = 0.99\r\nINTERPOLATION_PARAMETER = 0.001\r\nSTART_EPSILON = 1.0\r\nEPSILON_DECAY = 0.995\r\nFINAL_EPSILON = 0.01\r\nN_TRAINING_EPISODES = 2_000\r\nN_PLAYING_EPISODES = 10\r\nREPLAY_MEMORY_SIZE = 100_000\r\nREPLAY_MEMORY_BATCH_SIZE = 100\r\nSEED = 42\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py b/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py
--- a/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py	(date 1738782667802)
@@ -6,6 +6,6 @@
 FINAL_EPSILON = 0.01
 N_TRAINING_EPISODES = 2_000
 N_PLAYING_EPISODES = 10
-REPLAY_MEMORY_SIZE = 100_000
-REPLAY_MEMORY_BATCH_SIZE = 100
+REPLAY_MEMORY_SIZE = 10_000
+REPLAY_MEMORY_BATCH_SIZE = 64
 SEED = 42
Index: q_learning_taxi_env_agent/player/training_player.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom ..agent import QLearningAgent, Agent\r\nfrom ..environment import Environment\r\nfrom .player import Player\r\n\r\n\r\nclass TrainingPlayer(Player):\r\n    def __init__(self, agent: Agent, env: Environment, n_episodes : int = None):\r\n        super().__init__(agent, env, n_episodes)\r\n\r\n    def play(self):\r\n        episode_count = 0\r\n\r\n        while self.n_episodes is None or episode_count < self.n_episodes:\r\n            obs, info = self.env.reset(self.env.seed)\r\n            done = False\r\n\r\n            while not done:\r\n                action = self.agent.get_action(obs)\r\n                next_obs, reward, terminated, truncated, info = self.env.step(action)\r\n                self.agent.update(obs, action, reward, terminated, next_obs)\r\n                obs = next_obs\r\n                done = terminated or truncated\r\n\r\n            self.agent.decay_epsilon()\r\n            episode_count += 1\r\n            self.progress_bar.update(1)\r\n\r\n        self.env.close()\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/player/training_player.py b/q_learning_taxi_env_agent/player/training_player.py
--- a/q_learning_taxi_env_agent/player/training_player.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/player/training_player.py	(date 1738783580550)
@@ -22,7 +22,7 @@
                 obs = next_obs
                 done = terminated or truncated
 
-            self.agent.decay_epsilon()
+            self.agent.end_of_episode_hook()
             episode_count += 1
             self.progress_bar.update(1)
 
Index: q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import cv2\r\nimport numpy as np\r\nimport torch\r\n\r\nfrom .compute_device import ComputeDevice\r\n\r\n\r\nclass FramePreprocessor:\r\n    def __init__(self):\r\n        self.device = ComputeDevice.get_device()\r\n\r\n    def preprocess(self, frame):\r\n        frame = cv2.resize(frame, (128,128), interpolation=cv2.INTER_AREA)\r\n        frame = frame.astype(np.float32) / 255.0\r\n        frame = np.transpose(frame, (2, 0, 1))\r\n\r\n        frame = torch.tensor(frame, dtype=torch.float32, device=self.device).unsqueeze(0)\r\n        return frame
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py b/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py
--- a/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py	(date 1738526641357)
@@ -14,5 +14,4 @@
         frame = frame.astype(np.float32) / 255.0
         frame = np.transpose(frame, (2, 0, 1))
 
-        frame = torch.tensor(frame, dtype=torch.float32, device=self.device).unsqueeze(0)
         return frame
\ No newline at end of file
Index: q_learning_taxi_env_agent/constans/constants.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nclass Constants:\r\n    PROGRESS_MEMORY_DIRECTORY = \"./q_learning_taxi_env_agent/progress_memory/\"\r\n    PLAYER_VIDEOS_DIRECTORY = \"./q_learning_taxi_env_agent/player_videos/\"\r\n    PLAYER_NAME_PREFIX = \"player\"\r\n    ENVIRONMENT_ID_TAXI_V3 = \"Taxi-v3\"\r\n    ENVIRONMENT_LUNAR_LANDER_V3 = \"LunarLander-v3\"\r\n    ENVIRONMENT_ALE_MS_PACMAN_V5 = \"ALE/MsPacman-v5\"
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/constans/constants.py b/q_learning_taxi_env_agent/constans/constants.py
--- a/q_learning_taxi_env_agent/constans/constants.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/constans/constants.py	(date 1738782946782)
@@ -5,4 +5,4 @@
     PLAYER_NAME_PREFIX = "player"
     ENVIRONMENT_ID_TAXI_V3 = "Taxi-v3"
     ENVIRONMENT_LUNAR_LANDER_V3 = "LunarLander-v3"
-    ENVIRONMENT_ALE_MS_PACMAN_V5 = "ALE/MsPacman-v5"
\ No newline at end of file
+    ENVIRONMENT_ALE_MS_PACMAN_V5 = "MsPacmanNoFrameskip-v4"
\ No newline at end of file
Index: q_learning_taxi_env_agent/environment/envrionment.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from abc import ABC\r\n\r\nimport gymnasium as gym\r\n\r\nfrom ..render_modes import RenderMode\r\n\r\n\r\nclass Environment:\r\n\r\n    def __init__(self, env_id: str, render_mode: RenderMode = RenderMode.NONE, seed: int = None):\r\n        self.env_id = env_id\r\n        self.render_mode = render_mode\r\n        self.seed = seed\r\n        self.inner_env = gym.make(env_id, render_mode=render_mode.value)\r\n\r\n    def step(self, action):\r\n        return self.inner_env.step(action)\r\n\r\n    def reset(self, seed: int = None):\r\n        return self.inner_env.reset(seed=seed)\r\n\r\n    def get_action_space(self):\r\n        return self.inner_env.action_space\r\n\r\n    def random_step(self):\r\n        return self.inner_env.action_space.sample()\r\n\r\n    def get_observation_space(self):\r\n        return self.inner_env.observation_space\r\n\r\n    def close(self):\r\n        return self.inner_env.close()\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/environment/envrionment.py b/q_learning_taxi_env_agent/environment/envrionment.py
--- a/q_learning_taxi_env_agent/environment/envrionment.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/environment/envrionment.py	(date 1738621369357)
@@ -1,17 +1,26 @@
-from abc import ABC
 
 import gymnasium as gym
+import ale_py
+
 
 from ..render_modes import RenderMode
 
 
 class Environment:
 
-    def __init__(self, env_id: str, render_mode: RenderMode = RenderMode.NONE, seed: int = None):
+    def __init__(self, env_id: str, render_mode: RenderMode = RenderMode.NONE, seed: int = None, number_of_envs: int = None):
+        gym.register_envs(ale_py)
+
         self.env_id = env_id
         self.render_mode = render_mode
         self.seed = seed
-        self.inner_env = gym.make(env_id, render_mode=render_mode.value)
+        self.number_of_envs = number_of_envs
+
+        if number_of_envs is None or number_of_envs == 1:
+            self.inner_env = gym.make(env_id, render_mode=render_mode.value)
+        else:
+            self.inner_env = gym.make_vec(env_id, num_envs=number_of_envs, vectorization_mode="async", render_mode=render_mode.value)
+
 
     def step(self, action):
         return self.inner_env.step(action)
Index: q_learning_taxi_env_agent/agent/network/A3C_network.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/network/A3C_network.py b/q_learning_taxi_env_agent/agent/network/A3C_network.py
new file mode 100644
--- /dev/null	(date 1738784040055)
+++ b/q_learning_taxi_env_agent/agent/network/A3C_network.py	(date 1738784040055)
@@ -0,0 +1,38 @@
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as functional
+
+from ...hyper_parameters.A3C_hyper_parameters import SEED
+
+class A3CNetwork(nn.Module):
+    def __init__(self, action_space_size: int, seed: int = SEED):
+        super().__init__()
+        self.seed = torch.manual_seed(seed)
+        self.action_space_size = action_space_size
+
+        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)
+        self.bn1 = nn.BatchNorm2d(32)
+        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
+        self.bn2 = nn.BatchNorm2d(64)
+        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
+        self.bn3 = nn.BatchNorm2d(64)
+        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)
+        self.bn4 = nn.BatchNorm2d(128)
+        self.fc1 = nn.Linear(12_800, 512)
+        self.fc2 = nn.Linear(512, 256)
+        self.fc3a = nn.Linear(256, self.action_space_size)
+        self.fc3s = nn.Linear(256, 1)
+
+
+    def forward(self, obs):
+        x = functional.relu(self.bn1(self.conv1(obs)))
+        x = functional.relu(self.bn2(self.conv2(x)))
+        x = functional.relu(self.bn3(self.conv3(x)))
+        x = functional.relu(self.bn4(self.conv4(x)))
+        x = torch.flatten(x, start_dim=1)
+        x = functional.relu(self.fc1(x))
+        x = functional.relu(self.fc2(x))
+        action_values = self.fc3a(x)
+        state_value = self.fc3s(x)
+        return action_values, state_value
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"2117307c-a555-48c0-8362-23c29dbf44f9\" name=\"Changes\" comment=\"Refactored the player structure and clean up agent specific constants\">\r\n      <change afterPath=\"$PROJECT_DIR$/main5.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/main6.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/convolutional_deep_q_learning_network.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/convolutional_deep_q_learning_constants.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/main3.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/main3.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/agent.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/agent.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/convolutiona_deep_q_learning_network.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/main4.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/deep_q_learning_network.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/deep_q_learning_network.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/q_learning_agent.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/q_learning_agent.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/experience_handler.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/experience_handler.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/replay_memory.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/replay_memory.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/constants.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/constants.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/deep_q_learning_constants.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/constans/deep_q_learning_constants.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/environment/statistics_recording_environment.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/environment/statistics_recording_environment.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/environment/video_recording_environment.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/environment/video_recording_environment.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/__init__.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/__init__.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/player/player.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/q_learning_taxi_env_agent/player/player.py\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 3\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2q8K3tsM1ha6sYVMhUAkvbAYjGm\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\"><![CDATA[{\r\n  \"keyToString\": {\r\n    \"Python.blackjack.executor\": \"Debug\",\r\n    \"Python.main (1).executor\": \"Run\",\r\n    \"Python.main.executor\": \"Debug\",\r\n    \"Python.main1.executor\": \"Run\",\r\n    \"Python.main2.executor\": \"Run\",\r\n    \"Python.main3.executor\": \"Run\",\r\n    \"Python.main4.executor\": \"Run\",\r\n    \"Python.main5.executor\": \"Run\",\r\n    \"Python.q_learning_taxi_env.executor\": \"Run\",\r\n    \"Python.test.executor\": \"Debug\",\r\n    \"RunOnceActivity.ShowReadmeOnStart\": \"true\",\r\n    \"git-widget-placeholder\": \"master\",\r\n    \"last_opened_file_path\": \"E:/Documents/programming/aritificial intelligence/q-learning-thesis\",\r\n    \"settings.editor.selected.configurable\": \"preferences.keymap\"\r\n  }\r\n}]]></component>\r\n  <component name=\"RecentsManager\">\r\n    <key name=\"CopyFile.RECENT_KEYS\">\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\" />\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\constans\" />\r\n    </key>\r\n    <key name=\"MoveFile.RECENT_KEYS\">\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\constans\" />\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\agent\\utils\" />\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\hyper_parameters\" />\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\agent\\policy\" />\r\n      <recent name=\"E:\\Documents\\programming\\aritificial intelligence\\q-learning-thesis\\q_learning_taxi_env_agent\\utils\" />\r\n    </key>\r\n  </component>\r\n  <component name=\"RunManager\" selected=\"Python.main5\">\r\n    <configuration name=\"main\" type=\"PythonConfigurationType\" factoryName=\"Python\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main1\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main1.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main2\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main2.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main3\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main3.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main4\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main4.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"main5\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"q-learning-thesis\" />\r\n      <option name=\"ENV_FILES\" value=\"\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/main5.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <list>\r\n      <item itemvalue=\"Python.main\" />\r\n      <item itemvalue=\"Python.main4\" />\r\n      <item itemvalue=\"Python.main5\" />\r\n      <item itemvalue=\"Python.main3\" />\r\n      <item itemvalue=\"Python.main1\" />\r\n      <item itemvalue=\"Python.main2\" />\r\n    </list>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.main5\" />\r\n        <item itemvalue=\"Python.main4\" />\r\n        <item itemvalue=\"Python.main3\" />\r\n        <item itemvalue=\"Python.main1\" />\r\n        <item itemvalue=\"Python.main2\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-python-sdk-d7ad00fb9fc3-c546a90a8094-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-242.23726.102\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"2117307c-a555-48c0-8362-23c29dbf44f9\" name=\"Changes\" comment=\"\" />\r\n      <created>1734038566373</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1734038566373</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00001\" summary=\"Refactored the decorator pattern for the environment implementations to a simple property pattern\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1736187774096</created>\r\n      <option name=\"number\" value=\"00001\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1736187774096</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00002\" summary=\"Improved video recording environment added main scripts for training, inference and rendering\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1736891044382</created>\r\n      <option name=\"number\" value=\"00002\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1736891044382</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00003\" summary=\"Implemented Deep Q Learning player and solving the environment\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1737834109657</created>\r\n      <option name=\"number\" value=\"00003\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1737834109657</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00004\" summary=\"Refactored the player structure and clean up agent specific constants\">\r\n      <option name=\"closed\" value=\"true\" />\r\n      <created>1737919144738</created>\r\n      <option name=\"number\" value=\"00004\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1737919144738</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"5\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"Vcs.Log.History.Properties\">\r\n    <option name=\"COLUMN_ID_ORDER\">\r\n      <list>\r\n        <option value=\"Default.Root\" />\r\n        <option value=\"Default.Author\" />\r\n        <option value=\"Default.Date\" />\r\n        <option value=\"Default.Subject\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State />\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <MESSAGE value=\"Refactored the decorator pattern for the environment implementations to a simple property pattern\" />\r\n    <MESSAGE value=\"Improved video recording environment added main scripts for training, inference and rendering\" />\r\n    <MESSAGE value=\"Implemented Deep Q Learning player and solving the environment\" />\r\n    <MESSAGE value=\"Refactored the player structure and clean up agent specific constants\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Refactored the player structure and clean up agent specific constants\" />\r\n  </component>\r\n  <component name=\"XDebuggerManager\">\r\n    <breakpoint-manager>\r\n      <breakpoints>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/main1.py</url>\r\n          <line>11</line>\r\n          <option name=\"timeStamp\" value=\"1\" />\r\n        </line-breakpoint>\r\n      </breakpoints>\r\n      <default-breakpoints>\r\n        <breakpoint type=\"python-exception\">\r\n          <properties notifyOnTerminate=\"true\" exception=\"BaseException\">\r\n            <option name=\"notifyOnTerminate\" value=\"true\" />\r\n          </properties>\r\n        </breakpoint>\r\n      </default-breakpoints>\r\n    </breakpoint-manager>\r\n    <watches-manager>\r\n      <configuration name=\"PythonConfigurationType\">\r\n        <watch expression=\"self.env\" language=\"Python\" />\r\n      </configuration>\r\n    </watches-manager>\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/.idea/workspace.xml	(date 1738791553044)
@@ -4,34 +4,23 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="2117307c-a555-48c0-8362-23c29dbf44f9" name="Changes" comment="Refactored the player structure and clean up agent specific constants">
-      <change afterPath="$PROJECT_DIR$/main5.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/main6.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/convolutional_deep_q_learning_network.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/convolutional_deep_q_learning_constants.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py" afterDir="false" />
+    <list default="true" id="2117307c-a555-48c0-8362-23c29dbf44f9" name="Changes" comment="Implemented Convolutional Deep Q Learning agent for the Ms Pacman environment">
+      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/A3C_agent.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/A3C_network.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/A3C_hyper_parameters.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/main3.py" beforeDir="false" afterPath="$PROJECT_DIR$/main3.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/__init__.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/__init__.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/agent.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/agent.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/convolutional_deep_q_learning_agent.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/__init__.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/convolutiona_deep_q_learning_network.py" beforeDir="false" afterPath="$PROJECT_DIR$/main4.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/deep_q_learning_network.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/network/deep_q_learning_network.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/q_learning_agent.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/q_learning_agent.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/__init__.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/experience_handler.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/experience_handler.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/replay_memory.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/replay_memory.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/__init__.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/agent/utils/frame_preprocessor.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/constants.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/constants.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/deep_q_learning_constants.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/constans/deep_q_learning_constants.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/statistics_recording_environment.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/statistics_recording_environment.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/video_recording_environment.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/video_recording_environment.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/envrionment.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/environment/envrionment.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/__init__.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/__init__.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/player/player.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/player/player.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/hyper_parameters/convolutional_deep_q_learning_hyper_parameters.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/q_learning_taxi_env_agent/player/training_player.py" beforeDir="false" afterPath="$PROJECT_DIR$/q_learning_taxi_env_agent/player/training_player.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/test.py" beforeDir="false" afterPath="$PROJECT_DIR$/test.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -57,24 +46,25 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent"><![CDATA[{
-  "keyToString": {
-    "Python.blackjack.executor": "Debug",
-    "Python.main (1).executor": "Run",
-    "Python.main.executor": "Debug",
-    "Python.main1.executor": "Run",
-    "Python.main2.executor": "Run",
-    "Python.main3.executor": "Run",
-    "Python.main4.executor": "Run",
-    "Python.main5.executor": "Run",
-    "Python.q_learning_taxi_env.executor": "Run",
-    "Python.test.executor": "Debug",
-    "RunOnceActivity.ShowReadmeOnStart": "true",
-    "git-widget-placeholder": "master",
-    "last_opened_file_path": "E:/Documents/programming/aritificial intelligence/q-learning-thesis",
-    "settings.editor.selected.configurable": "preferences.keymap"
+  <component name="PropertiesComponent">{
+  &quot;keyToString&quot;: {
+    &quot;Python.blackjack.executor&quot;: &quot;Debug&quot;,
+    &quot;Python.main (1).executor&quot;: &quot;Run&quot;,
+    &quot;Python.main.executor&quot;: &quot;Debug&quot;,
+    &quot;Python.main1.executor&quot;: &quot;Run&quot;,
+    &quot;Python.main2.executor&quot;: &quot;Run&quot;,
+    &quot;Python.main3.executor&quot;: &quot;Run&quot;,
+    &quot;Python.main4.executor&quot;: &quot;Run&quot;,
+    &quot;Python.main5.executor&quot;: &quot;Run&quot;,
+    &quot;Python.main6.executor&quot;: &quot;Run&quot;,
+    &quot;Python.q_learning_taxi_env.executor&quot;: &quot;Run&quot;,
+    &quot;Python.test.executor&quot;: &quot;Run&quot;,
+    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
+    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
+    &quot;last_opened_file_path&quot;: &quot;E:/Documents/programming/aritificial intelligence/q-learning-thesis&quot;,
+    &quot;settings.editor.selected.configurable&quot;: &quot;preferences.keymap&quot;
   }
-}]]></component>
+}</component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
       <recent name="E:\Documents\programming\aritificial intelligence\q-learning-thesis" />
@@ -88,7 +78,7 @@
       <recent name="E:\Documents\programming\aritificial intelligence\q-learning-thesis\q_learning_taxi_env_agent\utils" />
     </key>
   </component>
-  <component name="RunManager" selected="Python.main5">
+  <component name="RunManager" selected="Python.main6">
     <configuration name="main" type="PythonConfigurationType" factoryName="Python" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
@@ -111,7 +101,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main1" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="main2" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -124,7 +114,7 @@
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main1.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main2.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -133,7 +123,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main2" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="main4" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -146,7 +136,7 @@
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main2.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main4.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -155,7 +145,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main3" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="main5" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -168,7 +158,7 @@
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main3.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main5.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -177,7 +167,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main4" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="main6" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -190,7 +180,7 @@
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main4.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main6.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -199,7 +189,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="main5" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="test" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="q-learning-thesis" />
       <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
@@ -212,7 +202,7 @@
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main5.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/test.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -223,18 +213,18 @@
     </configuration>
     <list>
       <item itemvalue="Python.main" />
+      <item itemvalue="Python.main6" />
+      <item itemvalue="Python.test" />
       <item itemvalue="Python.main4" />
       <item itemvalue="Python.main5" />
-      <item itemvalue="Python.main3" />
-      <item itemvalue="Python.main1" />
       <item itemvalue="Python.main2" />
     </list>
     <recent_temporary>
       <list>
+        <item itemvalue="Python.main6" />
+        <item itemvalue="Python.main4" />
         <item itemvalue="Python.main5" />
-        <item itemvalue="Python.main4" />
-        <item itemvalue="Python.main3" />
-        <item itemvalue="Python.main1" />
+        <item itemvalue="Python.test" />
         <item itemvalue="Python.main2" />
       </list>
     </recent_temporary>
@@ -287,7 +277,15 @@
       <option name="project" value="LOCAL" />
       <updated>1737919144738</updated>
     </task>
-    <option name="localTasksCounter" value="5" />
+    <task id="LOCAL-00005" summary="Implemented Convolutional Deep Q Learning agent for the Ms Pacman environment">
+      <option name="closed" value="true" />
+      <created>1738522547924</created>
+      <option name="number" value="00005" />
+      <option name="presentableId" value="LOCAL-00005" />
+      <option name="project" value="LOCAL" />
+      <updated>1738522547924</updated>
+    </task>
+    <option name="localTasksCounter" value="6" />
     <servers />
   </component>
   <component name="Vcs.Log.History.Properties">
@@ -316,7 +314,8 @@
     <MESSAGE value="Improved video recording environment added main scripts for training, inference and rendering" />
     <MESSAGE value="Implemented Deep Q Learning player and solving the environment" />
     <MESSAGE value="Refactored the player structure and clean up agent specific constants" />
-    <option name="LAST_COMMIT_MESSAGE" value="Refactored the player structure and clean up agent specific constants" />
+    <MESSAGE value="Implemented Convolutional Deep Q Learning agent for the Ms Pacman environment" />
+    <option name="LAST_COMMIT_MESSAGE" value="Implemented Convolutional Deep Q Learning agent for the Ms Pacman environment" />
   </component>
   <component name="XDebuggerManager">
     <breakpoint-manager>
Index: q_learning_taxi_env_agent/agent/A3C_agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/A3C_agent.py b/q_learning_taxi_env_agent/agent/A3C_agent.py
new file mode 100644
--- /dev/null	(date 1738790304302)
+++ b/q_learning_taxi_env_agent/agent/A3C_agent.py	(date 1738790304302)
@@ -0,0 +1,64 @@
+import numpy as np
+import torch
+import torch.nn.functional as functional
+import torch.optim as optim
+
+from .agent import Agent
+from ..environment import Environment
+from .utils import ComputeDevice, FramePreprocessor
+from .network import A3CNetwork
+
+
+class A3CAgent(Agent):
+    def __init__(
+            self,
+            env: Environment,
+            learning_rate: float
+    ):
+        super().__init__(env)
+        self.learning_rate = learning_rate
+        self.device = ComputeDevice.get_device()
+        self.action_space = self.env.get_action_space().n
+        self.network = A3CNetwork(self.action_space).to(device=self.device)
+        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learning_rate)
+        self.frame_preprocessor = FramePreprocessor()
+
+
+    def get_action(self, obs):
+        obs = self.frame_preprocessor.preprocess(obs)
+        obs = torch.tensor(np.array(obs), dtype=torch.float32, device=self.device).unsqueeze(0)
+        self.network.eval()
+        with torch.no_grad():
+            action_values, _ = self.network(obs)
+        self.network.train()
+
+        policy = functional.softmax(action_values, dim=-1)
+        action_indices = torch.multinomial(policy, 1).squeeze()
+        return action_indices.cpu().numpy()
+
+
+    def get_best_action(self, obs):
+        obs = self.frame_preprocessor.preprocess(obs)
+        obs = torch.tensor(np.array(obs), dtype=torch.float32, device=self.device).unsqueeze(0)
+        self.network.eval()
+        with torch.no_grad():
+            action_values, _ = self.network(obs)
+        self.network.train()
+
+        best_action_indices =  torch.argmax(action_values, dim=1).squeeze()
+
+        return  best_action_indices.cpu().numpy()
+
+
+
+def update(self, obs, action, reward, terminated, next_obs):
+        pass
+
+    def decay_epsilon(self):
+        pass
+
+    def load_progress(self, file_name: str):
+        pass
+
+    def save_progress(self, file_name: str):
+        pass
\ No newline at end of file
Index: q_learning_taxi_env_agent/agent/q_learning_agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\r\nfrom typing import override\r\n\r\nfrom .agent import Agent\r\nfrom ..environment import Environment\r\nfrom ..constans import Constants\r\n\r\nclass QLearningAgent(Agent):\r\n\r\n    def __init__(self,\r\n            env: Environment,\r\n            learning_rate: float,\r\n            start_epsilon: float,\r\n            epsilon_decay: float,\r\n            final_epsilon: float,\r\n            discount_factor: float):\r\n        super().__init__(env)\r\n        self.q_values = {state: np.zeros(self.env.get_action_space().n) for state in\r\n                         range(self.env.get_observation_space().n)}\r\n\r\n        self.learning_rate = learning_rate\r\n        self.epsilon = start_epsilon\r\n        self.epsilon_decay = epsilon_decay\r\n        self.final_epsilon = final_epsilon\r\n        self.discount_factor = discount_factor\r\n\r\n    def update(self, obs, action, reward, terminated, next_obs):\r\n        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\r\n        q_value = reward + (self.discount_factor * future_q_value)\r\n        temporal_difference = q_value - self.q_values[obs][action]\r\n        self.q_values[obs][action] += self.learning_rate * temporal_difference\r\n\r\n        self.training_error.append(temporal_difference)\r\n\r\n    def get_action(self, obs):\r\n        if np.random.random() < self.epsilon:\r\n            return self.env.random_step()\r\n        else:\r\n            return int(np.argmax(self.q_values[obs]))\r\n\r\n    def get_best_action(self, obs):\r\n        return int(np.argmax(self.q_values[obs]))\r\n\r\n    def save_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        q_values_str_keys = {str(k): v for k, v in self.q_values.items()}\r\n        np.savez(file_full_path, **q_values_str_keys)\r\n\r\n    def load_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        loaded_q_values = np.load(file_full_path)\r\n        self.q_values = {int(key): loaded_q_values[key] for key in loaded_q_values.keys()}\r\n\r\n    def decay_epsilon(self):\r\n        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/q_learning_agent.py b/q_learning_taxi_env_agent/agent/q_learning_agent.py
--- a/q_learning_taxi_env_agent/agent/q_learning_agent.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/q_learning_agent.py	(date 1738783557205)
@@ -54,4 +54,5 @@
     def decay_epsilon(self):
         self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)
 
-
+    def end_of_episode_hook(self):
+        self.decay_epsilon()
Index: q_learning_taxi_env_agent/hyper_parameters/A3C_hyper_parameters.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/hyper_parameters/A3C_hyper_parameters.py b/q_learning_taxi_env_agent/hyper_parameters/A3C_hyper_parameters.py
new file mode 100644
--- /dev/null	(date 1738618434531)
+++ b/q_learning_taxi_env_agent/hyper_parameters/A3C_hyper_parameters.py	(date 1738618434531)
@@ -0,0 +1,11 @@
+LEARNING_RATE = 0.0005
+DISCOUNT_FACTOR = 0.99
+INTERPOLATION_PARAMETER = 0.001
+START_EPSILON = 1.0
+EPSILON_DECAY = 0.995
+FINAL_EPSILON = 0.01
+N_TRAINING_EPISODES = 2_000
+N_PLAYING_EPISODES = 10
+REPLAY_MEMORY_SIZE = 25_000
+REPLAY_MEMORY_BATCH_SIZE = 100
+SEED = 42
Index: q_learning_taxi_env_agent/agent/agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom abc import ABC, abstractmethod\r\n\r\nfrom .utils import ExperienceHandler\r\nfrom ..environment import Environment\r\n\r\n\r\nclass Agent(ExperienceHandler, ABC):\r\n\r\n    def __init__(self, env: Environment):\r\n        self.env = env\r\n        self.training_error = []\r\n\r\n    @abstractmethod\r\n    def get_action(self, obs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def get_best_action(self, obs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def update(self, obs, action, reward, terminated, next_obs):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def decay_epsilon(self):\r\n        pass\r\n\r\n    def get_training_error(self):\r\n        return self.training_error
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/agent.py b/q_learning_taxi_env_agent/agent/agent.py
--- a/q_learning_taxi_env_agent/agent/agent.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/agent.py	(date 1738783580538)
@@ -23,8 +23,7 @@
     def update(self, obs, action, reward, terminated, next_obs):
         pass
 
-    @abstractmethod
-    def decay_epsilon(self):
+    def end_of_episode_hook(self):
         pass
 
     def get_training_error(self):
Index: q_learning_taxi_env_agent/agent/deep_q_learning_agent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing_extensions import override\r\n\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as functional\r\nimport torch.optim as optim\r\n\r\nfrom .agent import Agent\r\nfrom .network import DeepQLearningNetwork\r\nfrom .utils import ReplayMemory, ComputeDevice\r\nfrom ..environment import Environment\r\nfrom ..constans import Constants\r\n\r\nclass DeepQLearningAgent(Agent):\r\n\r\n    def __init__(\r\n            self,\r\n            env: Environment,\r\n            learning_rate: float,\r\n            discount_factor: float,\r\n            interpolation_parameter: float,\r\n            start_epsilon: float,\r\n            epsilon_decay: float,\r\n            final_epsilon: float,\r\n            replay_memory: ReplayMemory\r\n    ):\r\n        super().__init__(env)\r\n        self.learning_rate = learning_rate\r\n        self.discount_factor = discount_factor\r\n        self.epsilon = start_epsilon\r\n        self.epsilon_decay = epsilon_decay\r\n        self.final_epsilon = final_epsilon\r\n        self.interpolation_parameter = interpolation_parameter\r\n        self.device = ComputeDevice.get_device()\r\n        self.observation_space_size = env.get_observation_space().shape[0]\r\n        self.action_space_size = env.get_action_space().n\r\n        self.q_network = DeepQLearningNetwork(self.observation_space_size, self.action_space_size).to(self.device)\r\n        self.target_q_network = DeepQLearningNetwork(self.observation_space_size, self.action_space_size).to(\r\n            self.device)\r\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\r\n        self.replay_memory = replay_memory\r\n\r\n    def get_action(self, obs):\r\n        obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\r\n        self.q_network.eval()\r\n        with torch.no_grad():\r\n            action_values = self.q_network(obs)\r\n        self.q_network.train()\r\n        if np.random.random() < self.epsilon:\r\n            return torch.randint(0, action_values.size(1), (1,)).item()\r\n        else:\r\n            return torch.argmax(action_values, dim=1).item()\r\n\r\n    def get_best_action(self, obs):\r\n        obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\r\n        self.q_network.eval()\r\n        with torch.no_grad():\r\n            action_values = self.q_network(obs)\r\n        self.q_network.train()\r\n        return torch.argmax(action_values, dim=1).item()\r\n\r\n    def update(self, obs, action, reward, terminated, next_obs):\r\n        self.replay_memory.append((obs, action, reward, terminated, next_obs))\r\n        if len(self.replay_memory.memory) > self.replay_memory.batch_size:\r\n            transitions = self.replay_memory.sample()\r\n            self.learn(transitions)\r\n            self.soft_update()\r\n\r\n    def learn(self, transitions):\r\n        obs_batch, actions_batch, reward_batch, terminated_batch, next_obs_batch = transitions\r\n        future_q_target_values, _ = self.target_q_network(next_obs_batch).detach().max(dim=1, keepdim=True)\r\n        q_target_values = reward_batch + ((~terminated_batch) * self.discount_factor * future_q_target_values)\r\n        q_values = self.q_network(obs_batch).gather(1, actions_batch)\r\n        loss = functional.mse_loss(q_values, q_target_values)\r\n        self.training_error.append(loss.item())\r\n        self.optimizer.zero_grad()\r\n        loss.backward()\r\n        self.optimizer.step()\r\n\r\n    def soft_update(self):\r\n        for target_parameter, local_parameter in zip(self.target_q_network.parameters(), self.q_network.parameters()):\r\n            target_parameter.data.copy_(\r\n                self.interpolation_parameter * local_parameter +\r\n                    (1.0 - self.interpolation_parameter) * target_parameter)\r\n\r\n    def save_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        torch.save(self.q_network.state_dict(), file_full_path)\r\n\r\n    def load_progress(self, file_name: str):\r\n        file_full_path = Constants.PROGRESS_MEMORY_DIRECTORY + file_name\r\n        self.q_network.load_state_dict(torch.load(file_full_path, weights_only=True))\r\n\r\n    def decay_epsilon(self):\r\n        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py b/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py
--- a/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/deep_q_learning_agent.py	(date 1738783580522)
@@ -93,3 +93,6 @@
 
     def decay_epsilon(self):
         self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)
+
+    def end_of_episode_hook(self):
+        self.decay_epsilon()
\ No newline at end of file
Index: q_learning_taxi_env_agent/hyper_parameters/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom .q_learning_hyper_parameters import *\r\nfrom .deep_q_learning_hyper_parameters import *\r\nfrom .convolutional_deep_q_learning_hyper_parameters import *
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/hyper_parameters/__init__.py b/q_learning_taxi_env_agent/hyper_parameters/__init__.py
--- a/q_learning_taxi_env_agent/hyper_parameters/__init__.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/hyper_parameters/__init__.py	(date 1738618434547)
@@ -1,4 +1,4 @@
-
 from .q_learning_hyper_parameters import *
 from .deep_q_learning_hyper_parameters import *
-from .convolutional_deep_q_learning_hyper_parameters import *
\ No newline at end of file
+from .convolutional_deep_q_learning_hyper_parameters import *
+from .A3C_hyper_parameters import *
\ No newline at end of file
Index: q_learning_taxi_env_agent/agent/network/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nfrom .deep_q_learning_network import DeepQLearningNetwork\r\nfrom .convolutional_deep_q_learning_network import ConvolutionalDeepQLearningNetwork
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/q_learning_taxi_env_agent/agent/network/__init__.py b/q_learning_taxi_env_agent/agent/network/__init__.py
--- a/q_learning_taxi_env_agent/agent/network/__init__.py	(revision 112f7b966a519a66ac1c1b2901593a7c60bacd36)
+++ b/q_learning_taxi_env_agent/agent/network/__init__.py	(date 1738784040069)
@@ -1,3 +1,4 @@
 
 from .deep_q_learning_network import DeepQLearningNetwork
-from .convolutional_deep_q_learning_network import ConvolutionalDeepQLearningNetwork
\ No newline at end of file
+from .convolutional_deep_q_learning_network import ConvolutionalDeepQLearningNetwork
+from .A3C_network import A3CNetwork
\ No newline at end of file
